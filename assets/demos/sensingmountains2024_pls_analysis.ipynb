{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4D change analysis of near-continuous LiDAR time series for applications in geomorphic monitoring\n",
    "\n",
    "... description of objective, data, tools ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and calculation of surface changes\n",
    "\n",
    "Prepare the analysis by compiling the list of files (epochs) and read the timestamps from the file names (format `YYMMDD_hhmmss`) into `datetime` objects. Use the point cloud files and timestamps to create a py4dgeo `SpatiotemporalAnalysis` object. For this you need to instantiate the M3C2 algorithm. You can use the point cloud file `170115_150816_aoi_50cm.laz` as core points. Explore the point cloud properties in CloudCompare: \n",
    "\n",
    "* Considering the available point density and surface characteristics, what would be a suitable cylinder radius for the distance calculation?\n",
    "* What would be a suitable approach to derive the surface normals in this topography and expected types of surface changes?\n",
    "\n",
    "Hint: In this flat topography and predominant provess of sand deposition and erosion, it can be suitable to orient the normals purely vertically. In this case, they do not need to be computed, and you can [customize the py4dgeo algorithm](https://py4dgeo.readthedocs.io/en/latest/customization.html#Changing-search-directions) accordingly.\n",
    "\n",
    "Use the first point cloud in the time series (list of files) as reference epoch. You can assume a registration error of 1.9 cm for the M3C2 distance calculation (cf. <a href=\"#references\">Vos et al., 2022</a>).\n",
    "\n",
    "Explore the spatiotemporal change information by visualizing the changes at a selected epoch and visualizing the time series at a selected location. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">First, we start by setting up the Python environment and data:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import os\n",
    "import numpy as np\n",
    "import py4dgeo\n",
    "from datetime import datetime\n",
    "\n",
    "# specify the data path\n",
    "data_path = 'pointclouds_riegl' #'path-to-data'\n",
    "\n",
    "# check if the specified path exists\n",
    "if not os.path.isdir(data_path):\n",
    "    print(f'ERROR: {data_path} does not exist')\n",
    "    print('Please specify the correct path to the data directory by replacing <path-to-data> above.')\n",
    "\n",
    "# sub-directory containing the point clouds\n",
    "pc_dir = data_path #os.path.join(data_path, 'pointclouds')\n",
    "\n",
    "# list of point clouds (time series)\n",
    "pc_list_raw = os.listdir(pc_dir)\n",
    "pc_list_raw[:5] # print the first elements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">In the list of point cloud files you can see that we have one laz file per epoch available. The file name contains the timestamp of the epoch, respectively, in format `YYMMDD_hhmmss`. To use this information for our analysis, we read the timestamp information from the file names into `datetime` objects.</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the timestamps from file names\n",
    "timestamps = []\n",
    "pc_list = []\n",
    "for f in pc_list_raw:\n",
    "    if not f.endswith('.laz'):\n",
    "        continue\n",
    "\n",
    "    pc_list.append(f)\n",
    "\n",
    "    # get the timestamp from the file name\n",
    "    timestamp_str = '_'.join(f.split('.')[0].split(' ')[-1].split('_')[:]) # yields YYMMDD_hhmmss\n",
    "\n",
    "    # convert string to datetime object\n",
    "    timestamp = datetime.strptime(timestamp_str, '%y%m%d_%H%M%S')\n",
    "    timestamps.append(timestamp)\n",
    "\n",
    "timestamps[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Now we use the point cloud files and timestamp information to create a `SpatiotemporalAnalysis` object</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = py4dgeo.SpatiotemporalAnalysis(f'./pls_rotmoos_riegl.zip', force=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">As reference epoch, we use the first epoch in our time series:</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the reference epoch\n",
    "reference_epoch_file = os.path.join(pc_dir, pc_list[0])\n",
    "\n",
    "# read the reference epoch and set the timestamp\n",
    "reference_epoch = py4dgeo.read_from_las(reference_epoch_file)\n",
    "reference_epoch.timestamp = timestamps[0]\n",
    "\n",
    "# set the reference epoch in the spatiotemporal analysis object\n",
    "analysis.reference_epoch = reference_epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">For epochs to be added, we now configure the M3C2 algorithm to derive the change values. We would like to set the normals purely vertically, so we define a customized computation of cylinder `directions`:</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify corepoints, here all points of the reference epoch\n",
    "analysis.corepoints = reference_epoch.cloud[::]\n",
    "\n",
    "# configure an M3C2 algorithm\n",
    "if 1:\n",
    "    analysis.m3c2 = py4dgeo.M3C2(cyl_radii=[0.25], normal_radii=[0.25], max_distance=1.0, registration_error=0.0)\n",
    "else:\n",
    "    # inherit from the M3C2 algorithm class to define a custom direction algorithm\n",
    "    class M3C2_Vertical(py4dgeo.M3C2):\n",
    "        def directions(self):\n",
    "            return np.array([0, 0, 1]) # vertical vector orientation\n",
    "        \n",
    "    # specify M3C2 parameters for our custom algorithm class\n",
    "    analysis.m3c2 = M3C2_Vertical(cyl_radii=(1.0,), max_distance=10.0, registration_error = 0.019)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Now we add all the other epochs with their timestamps:</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# create a list to collect epoch objects\n",
    "epochs = []\n",
    "for e, pc_file in enumerate(pc_list[1:]):\n",
    "    epoch_file = os.path.join(pc_dir, pc_file)\n",
    "    epoch = py4dgeo.read_from_las(epoch_file)\n",
    "    epoch.timestamp = timestamps[e]\n",
    "    epochs.append(epoch)\n",
    "    e+=1\n",
    "\n",
    "# add epoch objects to the spatiotemporal analysis object\n",
    "analysis.add_epochs(*epochs)\n",
    "\n",
    "# print the spatiotemporal analysis data for 3 corepoints and 5 epochs, respectively\n",
    "print(f\"Space-time distance array:\\n{analysis.distances[:3,:5]}\")\n",
    "print(f\"Uncertainties of M3C2 distance calculation:\\n{analysis.uncertainties['lodetection'][:3, :5]}\")\n",
    "print(f\"Timestamp deltas of analysis:\\n{analysis.timedeltas[:5]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">We visualize the changes in the scene for a selected epoch, together with the time series of surface changes at a selected location. The location here was selected separately in CloudCompare (as the corepoint id).</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_idx_sel = 1027626 # selected core point index\n",
    "epoch_idx_sel = 10 # selected epoch index\n",
    "\n",
    "# import plotting module\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# allow interactive rotation in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# create the figure\n",
    "fig=plt.figure(figsize=(15,5))\n",
    "ax1=fig.add_subplot(1,2,1)\n",
    "ax2=fig.add_subplot(1,2,2)\n",
    "\n",
    "# get the corepoints\n",
    "corepoints = analysis.corepoints.cloud\n",
    "\n",
    "# get change values of last epoch for all corepoints\n",
    "distances = analysis.distances\n",
    "distances_epoch = [d[epoch_idx_sel] for d in distances] \n",
    "\n",
    "# get the time series of changes at a specific core point locations\n",
    "coord_sel = analysis.corepoints.cloud[cp_idx_sel]\n",
    "timeseries_sel = distances[cp_idx_sel]\n",
    "\n",
    "# get the list of timestamps from the reference epoch timestamp and timedeltas\n",
    "timestamps = [t + analysis.reference_epoch.timestamp for t in analysis.timedeltas]\n",
    "\n",
    "# plot the scene\n",
    "d = ax1.scatter(corepoints[:,0], corepoints[:,1], c=distances_epoch[:], cmap='seismic_r', vmin=-1.5, vmax=1.5, s=1, zorder=1) \n",
    "plt.colorbar(d, format=('%.2f'), label='Distance [m]', ax=ax1, pad=.15)\n",
    "\n",
    "# add the location of the selected coordinate\n",
    "ax1.scatter(coord_sel[0], coord_sel[1], facecolor='yellow', edgecolor='black', s=100, zorder=2, label='Selected location', marker='*')\n",
    "ax1.legend()\n",
    "\n",
    "# configure the plot layout\n",
    "ax1.set_xlabel('X [m]')\n",
    "ax1.set_ylabel('Y [m]')\n",
    "ax1.set_aspect('equal')\n",
    "ax1.set_title('Changes at %s' % (analysis.reference_epoch.timestamp+analysis.timedeltas[epoch_idx_sel]))\n",
    "\n",
    "# plot the time series\n",
    "ax2.scatter(timestamps, timeseries_sel, s=5, color='black', label='Raw')\n",
    "ax2.plot(timestamps, timeseries_sel, color='blue', label='Smoothed')\n",
    "ax2.set_xlabel('Date')\n",
    "\n",
    "# add the epoch of the plotted scene\n",
    "ax2.scatter(timestamps[epoch_idx_sel], timeseries_sel[epoch_idx_sel], facecolor='yellow', marker='*', edgecolor='black', s=100, color='red', label='Selected epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# format the date labels\n",
    "import matplotlib.dates as mdates\n",
    "dtFmt = mdates.DateFormatter('%b-%d') \n",
    "plt.gca().xaxis.set_major_formatter(dtFmt) \n",
    "\n",
    "# configure the plot layout\n",
    "ax2.set_ylabel('Distance [m]')\n",
    "ax2.grid()\n",
    "ax2.set_ylim(-0.3,1.6)\n",
    "ax2.set_title('Time series at selected location')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">The map of changes in the scene shows us linear structures of sand deposition near the coast which can be interpreted as sand bars (with knowledge about coastal processes). This is confirmed by the surface behavior over time, expressed in the time series plot. However, the time series is quite noisy especially in this part of the beach, which is regularly covered by water during high tides (leading to missing data) and also varies strongly in surface moisture (influencing the LiDAR range measurement and causing noise). We therefore continue with temporal smoothing of the time series.</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal smoothing\n",
    "\n",
    "You are dealing with a temporal subset of the original hourly time series. The effect of temporal measurement variability may therefore be less pronounced (compared to the assessment in, e.g., [Anders et al., 2019](#references)). Nonetheless, you may apply temporal smoothing to reduce the influence of noise on your change analysis using a rolling median averaging of one week. This will also fill possible gaps in your data, e.g., lower ranges during poor atmospheric conditions or no data due to water coverage during tides on the lower beach part. \n",
    "\n",
    "Visualize the raw and smoothed change values in the time series of your selected location."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">We apply a rolling median with a defined temporal window of 14 epochs (corresponding to one week of 12-hourly point clouds) using the `temporal_averaging()` function in py4dgeo.</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.smoothed_distances = py4dgeo.temporal_averaging(analysis.distances, smoothing_window=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Now we can compare the raw and smoothed time series at our selected location:</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the figure\n",
    "fig, ax = plt.subplots(1,1,figsize=(7,5))\n",
    "\n",
    "# plot the raw time series\n",
    "ax.scatter(timestamps, timeseries_sel, color='blue', label='raw', s=5)\n",
    "\n",
    "# plot the smoothed time series\n",
    "timeseries_sel_smooth = analysis.smoothed_distances[cp_idx_sel]\n",
    "ax.plot(timestamps, timeseries_sel_smooth, color='red', label='smooth')\n",
    "\n",
    "# format the date labels\n",
    "import matplotlib.dates as mdates\n",
    "dtFmt = mdates.DateFormatter('%b-%d') \n",
    "plt.gca().xaxis.set_major_formatter(dtFmt) \n",
    "\n",
    "# add plot elements\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Distance [m]')\n",
    "ax.grid()\n",
    "ax.set_ylim(-.25,1.25)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">From the smoothed time series at the selected location, we can now much better interpret the surface behavior. In fact, we can distinctly observe that there were two consecutive occurrences of temporary deposition of several weeks. These represent two phases where sand bars are present. They can be extracted as individual objects by the 4D objects-by-change method. Before, we continue with time series clustering and the assessment of overall change patterns in the following.</span> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series clustering\n",
    "To derive characteristic change patterns on the sandy beach, perform k-means clustering of the time series following <a href=\"#references\">Kuschnerus et al. (2021)</a>. Assess the clustering for different selections of `k` numbers of clusters.\n",
    "\n",
    "* Can you interpret the characteristics of different parts on the beach? Visualize example time series for different clusters.\n",
    "* From which number of clusters do you see a clear separation in overall units of the beach area?\n",
    "* What are some detail change patterns that become visible for a higher number of clusters?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">We perform k-means clustering for a set of `k`s at once and collect the resulting labels for subsequent assessment:</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kmeans clustering module from scikit-learn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# use the smoothed distances for clustering\n",
    "distances = analysis.smoothed_distances\n",
    "\n",
    "# define the number of clusters\n",
    "ks = [5,10,20,50]\n",
    "\n",
    "# create an array to store the labels\n",
    "labels = np.full((distances.shape[0], len(ks)), np.nan)\n",
    "\n",
    "# perform clustering for each number of clusters\n",
    "for kidx, k in enumerate(ks):\n",
    "    print(f'Performing clustering with k={k}...')\n",
    "    nan_indicator = np.logical_not(np.isnan(np.sum(distances, axis=1)))\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(distances[nan_indicator, :])\n",
    "    labels[nan_indicator, kidx] = kmeans.labels_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Now we can visualize the resulting change patterns for different numbers of clusters:</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,4, figsize=(12,7))\n",
    "(ax1,ax2,ax3,ax4) = axs\n",
    "\n",
    "cmap_clustering = 'tab20'\n",
    "sc1 = ax1.scatter(corepoints[:,0],corepoints[:,1],c=labels[:,0],cmap=cmap_clustering,s=1, label=ks[0])\n",
    "\n",
    "sc2 = ax2.scatter(corepoints[:,0],corepoints[:,1],c=labels[:,1],cmap=cmap_clustering,s=1, label=ks[1])\n",
    "\n",
    "sc3 = ax3.scatter(corepoints[:,0],corepoints[:,1],c=labels[:,2],cmap=cmap_clustering,s=1, label=ks[2])\n",
    "\n",
    "sc4 = ax4.scatter(corepoints[:,0],corepoints[:,1],c=labels[:,3],cmap=cmap_clustering,s=1, label=ks[3])\n",
    "\n",
    "ax_c = 0\n",
    "for ax in axs:\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(f'# clusters = {ks[ax_c]}')\n",
    "    ax_c+=1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">When using a small number of clusters (k=5), a large part of the beach is assigned to one cluster of assumingly little activity (gray area). From our exploration of changes in the scene at a selected epoch above, we further obtain two clusters with mainly deposition (blue clusters) and one cluster in the erosion areas around the pathway through the dunes. With a higher number of clusters (k=10 to k=50), the coarser clusters are further split up into (assumingly) finer patterns.</span> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><span style=\"color:blue\">We can look into the time series properties within selected clusters, to interpret the change pattern they are representing. Here, we will check the clusters derived with k=10 by plotting the median values of all time series per cluster:</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the figure\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,7))\n",
    "ax1,ax2 = axs\n",
    "\n",
    "# get the labels for the selected number of clusters\n",
    "labels_k = labels[:,1]\n",
    "\n",
    "# plot the map of clusters\n",
    "sc = ax1.scatter(corepoints[:,0],corepoints[:,1],c=labels_k,cmap=cmap_clustering,s=1)\n",
    "\n",
    "# plot the time series representing the clusters (median of each cluster)\n",
    "for l in np.unique(labels_k):\n",
    "    ax2.plot(timestamps, np.nanmedian(distances[labels_k==l,:], axis=0), label=f'cluster {l}')\n",
    "\n",
    "# format the date labels\n",
    "import matplotlib.dates as mdates\n",
    "dtFmt = mdates.DateFormatter('%b-%d')\n",
    "plt.gca().xaxis.set_major_formatter(dtFmt)\n",
    "\n",
    "# add plot elements\n",
    "ax1.set_aspect('equal')\n",
    "ax1.set_title(f'# clusters = {ks[1]}')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Distance [m]')\n",
    "ax2.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Now we can relate the temporal behavior to the spatial clusters. For example, we see strong and sudden surface increases for the orange and red clusters (with different timing and magnitude). The gray cluster (occuring in two spatial extents) represents sand bars, which we know from our time series example above. The large coverage of the brown cluster shows a time series with little activity - except for fluctuations, especially towards the end of the observation period, which is contained in all clusters. We can assume that this represents some measurement effects in the data that could not be corrected by the preprocessing and alignment procedure ([Vos et al., 2022]).</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Principal component analysis\n",
    "\n",
    "In the following, we will use principal component analysis (PCA; also referred to as empirical orthogonal functions) as a tool to summarize the change patterns within a spatiotemporal dataset. The method is based on [this master thesis](http://resolver.tudelft.nl/uuid:ce98c4e3-6ca1-4966-a5cf-2120f2fa44bf), to be published in [Frantzen et al. (2023)](#references). \n",
    " \n",
    "For this technique, the 4D data shall be represented as a two-dimensional array in which the rows correspond to different epochs, and the columns correspond to each individual [x, y] coordinate where a z or surface change value is represented. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reshape the data from the `SpatiotemporalAnalysis` object so that all coordinates are represented in one axis, i.e. an array holding [time, space]:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = distances.transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find coordinates without nan values, and create a new array without nan values:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idxs = np.argwhere(~np.isnan(X).any(axis=0)) # store the column indices where actual data is present\n",
    "X_no_nan = X[:,idxs.flatten()].reshape((X.shape[0],-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute the loading and scores of the prinicpal components in 'NaN-free' X:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "scores = pca.fit_transform(X_no_nan)\n",
    "result = pca.components_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the principal component loadings and scores along with their fraction of explained variance:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "coords_no_nan = analysis.corepoints.cloud[idxs]\n",
    "for i in range(1): #range(pca.n_components_):\n",
    "    fig, axes = plt.subplots(1,2, figsize=(10, 4))\n",
    "    sat_val = np.abs(np.nanpercentile(result[i], 10)) + np.abs(np.nanpercentile(result[i], 90)) / 2\n",
    "    s = axes[0].scatter(coords_no_nan[:,0,0], coords_no_nan[:,0,1], c=result[i], clim=(-sat_val,sat_val), cmap='PiYG')\n",
    "    axes[0].set_aspect('equal')\n",
    "    axes[1].plot(scores[i])\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Score')\n",
    "    fig.suptitle(f'PC loading (left) and scores (right) for principal component {i} with EVF {pca.explained_variance_ratio_[i]}')\n",
    "    plt.colorbar(s, ax=axes[0])\n",
    "    plt.tight_layout()\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of 4D objects-by-change\n",
    "\n",
    "Now use the 4D objects-by-change (4D-OBC) method to identify individual surface activities in the beach scene. The objective is to extract temporary occurrences of accumulation or erosion, as occurs when a sand bar is formed during some timespan, or when sand is deposited by anthropogenic works. This type of surface activity is implemented with the original seed detection in py4dgeo, so you do not need to customize the algorithm. Decide for suitable parameters following <a href=\"#references\">Anders et al. (2021)</a> - but bear in mind that we are using a different temporal resolution, so you may need to adjust the temporal change detection.\n",
    "\n",
    "Perform the extraction for selected seed locations, e.g. considering interesting clusters of change patterns identified in the previous step. In principle, the spatiotemporal segmentation can also be applied to the full dataset (all time series at all core point locations are used as potential seed candidates), but long processing time needs to be expected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">In this solution, we will use the selected location from above to extract the sand bars as 4D object-by-change. The strength of the method is that the occurrences are identified individually, even though they have spatial overlap, as they can be separated in the time series information.</span> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">We instantiate the `RegionGrowingAlgorithm` class using the parameters of [Anders et al, 2021](#references), and run the object extraction:</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrize the 4D-OBC extraction\n",
    "algo = py4dgeo.RegionGrowingAlgorithm(window_width=14, \n",
    "                                      minperiod=2, \n",
    "                                      height_threshold=0.05, \n",
    "                                      neighborhood_radius=1.0,\n",
    "                                      min_segments=10, \n",
    "                                      thresholds=[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], \n",
    "                                      seed_candidates=[cp_idx_sel])\n",
    "\n",
    "# run the algorithm\n",
    "analysis.invalidate_results(seeds=True, objects=True, smoothed_distances=False) # only required if you want to re-run the algorithm\n",
    "objects = algo.run(analysis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"></span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_timeseries = analysis.smoothed_distances[cp_idx_sel]\n",
    "plt.plot(timestamps,seed_timeseries, c='black', linestyle='--', linewidth=0.5, label='Seed timeseries')\n",
    "\n",
    "for sid, example_seed in enumerate(analysis.seeds):\n",
    "    seed_end = example_seed.end_epoch\n",
    "    seed_start = example_seed.start_epoch\n",
    "    seed_cp_idx = example_seed.index\n",
    "\n",
    "    plt.plot(timestamps[seed_start:seed_end+1], seed_timeseries[seed_start:seed_end+1], label=f'Seed {sid}')\n",
    "\n",
    "# format the date labels\n",
    "dtFmt = mdates.DateFormatter('%b-%d')\n",
    "plt.gca().xaxis.set_major_formatter(dtFmt)\n",
    "\n",
    "# add plot elements\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Distance [m]')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">At the selected location, three separate surface activities are detected as seed for 4D-OBC extraction. We may not be satisfied with the determination of the timing. In a real analysis, we would now aim to improve the temporal detection - either by using a different approach of seed detection (cf. [algorithm customization](https://py4dgeo.readthedocs.io/en/latest/4dobc-customization.html#Seed-point-detection)), or by postprocessing the temporal segments from the current seed detection.</span> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Here, we use the result and look into the spatial object properties:</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(15,7))\n",
    "ax1,ax2 = axs\n",
    "\n",
    "# get indices of 4D-OBC\n",
    "sel_seed_idx = 1\n",
    "idxs = objects[sel_seed_idx].indices\n",
    "\n",
    "# get change values at end of object for each location\n",
    "epoch_of_interest = int((objects[sel_seed_idx].end_epoch - objects[sel_seed_idx].start_epoch)/2 + objects[sel_seed_idx].start_epoch)\n",
    "distances_of_interest = analysis.smoothed_distances[:, epoch_of_interest]\n",
    "\n",
    "# get the change magnitude between end and start of object for each location\n",
    "magnitudes_of_interest = analysis.smoothed_distances[:, epoch_of_interest] - analysis.smoothed_distances[:, int(objects[sel_seed_idx].start_epoch)]\n",
    "\n",
    "# set the colormap according to magnitude at each location in the object\n",
    "crange = 1.0\n",
    "import matplotlib.colors as mcolors\n",
    "cmap = plt.get_cmap('seismic_r').copy()\n",
    "norm = mcolors.CenteredNorm(halfrange=crange)\n",
    "cmapvals = norm(magnitudes_of_interest)\n",
    "\n",
    "# plot the timeseries of the segmented locations (colored by time series similarity)\n",
    "for idx in idxs[::10]:\n",
    "    ax1.plot(timestamps, analysis.smoothed_distances[idx], c=cmap(cmapvals[idx]), linewidth=0.5)\n",
    "\n",
    "# plot the seed time series\n",
    "ax1.plot(timestamps, analysis.smoothed_distances[cp_idx_sel], c='black', linewidth=1., label='Seed timeseries')\n",
    "\n",
    "# fill the area of the object\n",
    "ax1.axvspan(timestamps[objects[sel_seed_idx].start_epoch], timestamps[objects[sel_seed_idx].end_epoch], alpha=0.3, color='grey', label='4D-OBC timespan')\n",
    "\n",
    "# add legend and format the date labels\n",
    "dtFmt = mdates.DateFormatter('%b-%d')\n",
    "plt.gca().xaxis.set_major_formatter(dtFmt)\n",
    "ax1.legend()\n",
    "\n",
    "# get subset of core points incorporated in 4D-OBC\n",
    "cloud = analysis.corepoints.cloud\n",
    "subset_cloud = cloud[idxs,:2]\n",
    "\n",
    "# plot coordinates colored by change values at end magnitude of object\n",
    "d = ax2.scatter(cloud[:,0], cloud[:,1], c = magnitudes_of_interest, cmap='seismic_r', vmin=-crange, vmax=crange, s=1)\n",
    "plt.colorbar(d, format=('%.2f'), label='Change magnitude [m]', ax=ax2)\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "# plot convex hull of 4D-OBC\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib.patches import Polygon\n",
    "hull = ConvexHull(subset_cloud)\n",
    "ax2.add_patch(Polygon(subset_cloud[hull.vertices,0:2], label = '4D-OBC hull', fill = False))\n",
    "\n",
    "# plot seed location of 4D-OBC\n",
    "ax2.scatter(cloud[cp_idx_sel,0], cloud[cp_idx_sel,1], marker = '*', c = 'black', label = 'Seed')\n",
    "\n",
    "# add plot elements\n",
    "ax1.set_title('Time series of segmented 4D-OBC locations')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Distance [m]')\n",
    "ax2.set_title(f'Magnitudes of change in the 4D-OBC timespan\\n({timestamps[epoch_of_interest]-timestamps[analysis.objects[sel_seed_idx].start_epoch]} hours)')\n",
    "ax2.set_xlabel('X [m]')\n",
    "ax2.set_ylabel('Y [m]')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">You may now check the different spatial extents for the three objects extracted from this seed location. In subsequent analysis, the spatial-temporal extent of each object can be used to describe individual activities (e.g., their change rates, magnitudes, ...) and relating them to one another in their timing and spatial distribution.</span> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "# References\n",
    "\n",
    "* Anders, K., Lindenbergh, R. C., Vos, S. E., Mara, H., de Vries, S., & Höfle, B. (2019). High-Frequency 3D Geomorphic Observation Using Hourly Terrestrial Laser Scanning Data Of A Sandy Beach. ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., IV-2/W5, pp. 317-324. doi: [10.5194/isprs-annals-IV-2-W5-317-2019](https://doi.org/10.5194/isprs-annals-IV-2-W5-317-2019).\n",
    "* Anders, K., Winiwarter, L., Mara, H., Lindenbergh, R., Vos, S. E., & Höfle, B. (2021). Fully automatic spatiotemporal segmentation of 3D LiDAR time series for the extraction of natural surface changes. ISPRS Journal of Photogrammetry and Remote Sensing, 173, pp. 297-308. doi: [10.1016/j.isprsjprs.2021.01.015](https://doi.org/10.1016/j.isprsjprs.2021.01.015).\n",
    "* Frantzen, P., Voordendag, A., Kuschnerus, M., Rutzinger, M.,  Wouters, B., Lindenbergh, R. (2023, in review). Identifying Paraglacial Geomorphic Process Dynamics Using A Principal Component Analysis Of Terrestrial Laser Scanning Time Series.\n",
    "* Kuschnerus, M., Lindenbergh, R., & Vos, S. (2021). Coastal change patterns from time series clustering of permanent laser scan data. Earth Surface Dynamics, 9 (1), pp. 89-103. doi: [10.5194/esurf-9-89-2021](https://doi.org/10.5194/esurf-9-89-2021).\n",
    "* Vos, S., Anders, K., Kuschnerus, M., Lindenberg, R., Höfle, B., Aarnikhof, S. & Vries, S. (2022). A high-resolution 4D terrestrial laser scan dataset of the Kijkduin beach-dune system, The Netherlands.  Scientific Data, 9:191. doi: [10.1038/s41597-022-01291-9](https://doi.org/10.1038/s41597-022-01291-9)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
